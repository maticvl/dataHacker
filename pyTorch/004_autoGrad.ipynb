{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "004_autoGrad.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maticvl/dataHacker/blob/master/pyTorch/004_autoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rte1f13L-qcW"
      },
      "source": [
        "![datahacker.png](data:image/png;base64,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)\r\n",
        "# Computation graphs and Autograd in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV_NWJXPS3O_"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iX__NJIIF1a"
      },
      "source": [
        "## Simple example with gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw-BWzT_BlsQ"
      },
      "source": [
        "Let's now see a simple example of how the derivative is calculated. We will create a scalar tensor `x` and set the `requires_grad` parameter to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOt6cjXjBmDb"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwk1w6AoVSk8"
      },
      "source": [
        "We will calculate `y` the following way:\\\r\n",
        "$ y = 3x^2 + 4x + 2$\r\n",
        "\r\n",
        "Now let's see what we get when we replace the `x` with our value `3`:  \r\n",
        "$ y = 3(3)^2 + 4(3) + 2 $\\\r\n",
        "$ y = 3*9 + 12 + 2$\\\r\n",
        "$ y = 27 + 12 + 2 $\\\r\n",
        "$ y = 41 $\r\n",
        "\\\r\n",
        "Now comes the part were we take the derivative of `y` with respect to the variable `x`.\\\r\n",
        "$\\frac{dy}{dx} = 2*3x + 4 = 6x + 4$\\\r\n",
        "If we replace `x` with our value `3`, we get the following:\r\n",
        "$6x + 4 = 6(3) + 4 = 18 + 4 = 22$\\\r\n",
        "So the gradient is equal to $22$\\\r\n",
        "Let's see how we can do this in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DRxYu1cB7Zc"
      },
      "source": [
        "y = 3*x**2 + 4*x + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb8Yg1rtCE8V"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPYtm2gx-kyp"
      },
      "source": [
        "y = x+3\r\n",
        "z = y**2 + 2\r\n",
        "z.backward()\r\n",
        "\r\n",
        "dz / dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRNnTnhs_Rb7"
      },
      "source": [
        "x = torch.tensor(3, requ... = )\r\n",
        "y = torch.tensor(3, requ... = )\r\n",
        "z = x * y\r\n",
        "\r\n",
        "z.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr97RVIfXHF5"
      },
      "source": [
        "Call `y.backward()` to calculate the derivative for that function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgm27RhCLkV"
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFdIjaXTCM9N"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGA2TTZ2a72l"
      },
      "source": [
        "## Is there a way to turn off the gradient calculation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3myJit2O93s"
      },
      "source": [
        "The answer is yes, you can turn the gradient calculation anytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlL9kf-UbEaQ"
      },
      "source": [
        "x = torch.tensor(1., requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcO3N1fMVYS0"
      },
      "source": [
        "x = x.requires_grad_(False)\r\n",
        "print(x_nograd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSQmsP8mVgfc"
      },
      "source": [
        "x = x.detach()\r\n",
        "print(x_nograd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFiwtMAiWX9_"
      },
      "source": [
        "# Gradient accumulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUcL2csoPiov"
      },
      "source": [
        "The auto gradient calculation does not reset the gradients automatically, therefore we have to reset them after each optimization. If we forget this step they could end up just accumulating.\\\r\n",
        "This sounds complex, but it is not, it's easy. To reset the gradients for a particular tensor, you can simply pass `x.grad.zero_()` and it will reset the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxLVo9P_WS2G"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\r\n",
        "\r\n",
        "for epoch in range(3):\r\n",
        "  y = 3*x**2 + 4*x + 2\r\n",
        "  y.backward()\r\n",
        "\r\n",
        "  print(x.grad)\r\n",
        "  # x.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEO9m6PElhrw"
      },
      "source": [
        "## Optimizing parameters with autograd "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33aJwXNpQ6QJ"
      },
      "source": [
        "Create `x` and `y` data and transform them to be torch tensors using the function `torch.from_numpy()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kmgpgY4Q1bv"
      },
      "source": [
        "x = np.array([1, 2, 3, 4, 5])\r\n",
        "y = 2 * x\r\n",
        "\r\n",
        "x_torch = torch.from_numpy(x)\r\n",
        "y_torch = torch.from_numpy(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-jlM9DcRE8N"
      },
      "source": [
        "Create a list for `w` and `b` in which we will save those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeJnLiyj7ac5"
      },
      "source": [
        "w_ = torch.tensor(5., requires_grad=True)\r\n",
        "b_ = torch.tensor(1., requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0crGHfd5RNNW"
      },
      "source": [
        "We will iterate and calculate the gradients for each value from the tensor `x_torch`. If we want to calculate the gradient for each number instead of the whole batch or list of numbers, we need to set the `w` and `b` parameters to the same number after calculation. After this step we do the forward pass for that number, and then we calculate the loss for that number and do the backpropagation. Update the `w` and `b` parameters and save them in the list we initialized earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EGIonTKRNcu"
      },
      "source": [
        "for i_value in range(len(y_torch)):\r\n",
        "  w = torch.tensor(5., requires_grad=True)\r\n",
        "  b = torch.tensor(1., requires_grad=True)\r\n",
        "\r\n",
        "  y_hat = w * x_torch[i_value] + b\r\n",
        "  \r\n",
        "  error = y_hat - y_torch[i_value]\r\n",
        "  loss = error ** 2 \r\n",
        "\r\n",
        "  loss.backward()\r\n",
        "  alpha = 0.01\r\n",
        "  with torch.no_grad():\r\n",
        "    w_.data -= alpha * w.grad\r\n",
        "    b_.data -= alpha * b.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSghVw7USY2q"
      },
      "source": [
        "To get the final gradient we need to sum all of the gradients for all the numbers and divide them with the total number of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9TIlFQfe47K"
      },
      "source": [
        "print(w_)\r\n",
        "print(b_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRi5BA-7SfyA"
      },
      "source": [
        "Let's now see how we can do the same thing but just with the whole set of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbMYWMiLIkS6"
      },
      "source": [
        "x_torch = torch.from_numpy(x)\r\n",
        "y_torch = torch.from_numpy(y)\r\n",
        "\r\n",
        "w_grad = []\r\n",
        "b_grad = []\r\n",
        "\r\n",
        "w = torch.tensor(5., requires_grad=True)\r\n",
        "b = torch.tensor(1., requires_grad=True)\r\n",
        "\r\n",
        "y_hat = w * x_torch + b\r\n",
        "\r\n",
        "error = y_hat - y_torch\r\n",
        "loss = error ** 2 \r\n",
        "loss = loss.mean()\r\n",
        "\r\n",
        "loss.backward()\r\n",
        "alpha = 0.01\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  w.data -= alpha * w.grad\r\n",
        "  b.data -=alpha * b.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1JgTpKleZec"
      },
      "source": [
        "print(w)\r\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}